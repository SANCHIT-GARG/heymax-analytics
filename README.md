# HeyMax Analytics Stack ğŸš€

This project sets up an end-to-end open-source data and analytics pipeline for HeyMax â€” built using:

- ğŸ£ DuckDB as the lightweight analytical warehouse
- ğŸ›  dbt for modeling (with staging, core, and growth layers)
- ğŸ“Š Streamlit for interactive growth dashboards
- âœ… All in a single GitHub repo â€” easily portable and reproducible

---

## ğŸ“¦ Project Structure

```
heymax_project/
â”œâ”€â”€ dashboard.py              # Streamlit app
â”œâ”€â”€ heymax.duckdb             # DuckDB file created by dbt
â”œâ”€â”€ requirements.txt          # All dependencies with versions
â”œâ”€â”€ README.md                 # You're reading it
â”‚
â”œâ”€â”€ heymax_analytics/         # dbt project folder
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/          # stg_events.sql (materialized as table to inspect raw data)
â”‚   â”‚   â”œâ”€â”€ core/             # dim_users, fct_events (materialized as parquet tables)
â”‚   â”‚   â””â”€â”€ growth/           # active users, growth accounting, cohort
â”‚   â””â”€â”€ models/schema.yml     # model docs + tests
â”‚
â””â”€â”€ .gitignore
```

---

## âš™ï¸ Setup Instructions

### 1. Clone and create virtual environment
```bash
git clone https://github.com/yourusername/heymax_project.git
cd heymax_project
python3 -m venv venv
source venv/bin/activate
```

### 2. Install Python dependencies
```bash
pip install -r requirements.txt
```

### 3. Run dbt models
```bash
cd heymax_analytics
dbt run
dbt test
```

### 4. Launch the dashboard
```bash
cd ..
streamlit run dashboard.py
```

---

## ğŸ§ª Features & Metrics

- **Daily / Weekly / Monthly Active Users**
- **Growth Accounting**
  - New Users
  - Retained
  - Resurrected
  - Churned
  - Quick Ratio (based on DAU; gaps indicate missing data)
- **Retention Cohort** heatmap with triangle format
- **Streamlit interactivity**
  - Date range filters
  - Metric selector (DAU/WAU/MAU-based growth accounting)
  - Country filter (added)
  - Line / Area chart toggles

---

## ğŸ“ˆ Performance Optimizations (DuckDB)

DuckDB handles most indexing internally, but you can improve performance by:

- âœ… **Materializing dbt models to Parquet** (e.g. fct_events, dim_users) for better read efficiency
- âœ… **Partitioning by `event_date` or `country`** if loading from files
- âœ… **Filtering with indexed columns** (e.g. user_id, event_date)
- âœ… **Using vectorized execution + stats pushdown** (automatic)

> Note: Raw staging table (`stg_events`) is intentionally materialized as table for inspection.

### ğŸ” Model Materialization Summary
| Model         | Materialization | Format  | Notes                            |
|---------------|------------------|---------|----------------------------------|
| stg_events    | table            | default | Retained for raw data inspection |
| dim_users     | table            | parquet | Efficient columnar access        |
| fct_events    | table            | parquet | Optimized for analytical queries |

### ğŸ“¦ Why Parquet?
Parquet is a columnar storage format that excels for analytics:
- ğŸš€ **Faster Reads** â€” only loads queried columns (vs. row-wise storage)
- ğŸ’¾ **Highly Compressed** â€” reduces storage footprint
- ğŸ” **Vectorized Execution** â€” better scan + filter performance in DuckDB
- ğŸ”„ **Interoperability** â€” widely supported by Spark, Pandas, BigQuery, etc.

---

## ğŸŒ Deployment (optional)

To deploy on [Streamlit Cloud](https://streamlit.io/cloud):
1. Push this repo to GitHub
2. Go to https://streamlit.io/cloud and link your GitHub repo
3. Set the entrypoint as `dashboard.py`
4. Done! ğŸ‰

---

## ğŸ”§ Requirements

### Python Version
- Python â‰¥ 3.9 (recommended)

### requirements.txt
```txt
streamlit==1.33.0
pandas==2.2.2
duckdb==0.10.1
altair==5.3.0
dbt-duckdb==1.9.0
watchdog==4.0.0
```

---

## ğŸ“ˆ Next Steps for Scaling

This stack is modular and built to grow. Here's how to take it to the next level:

| Upgrade Area          | Suggested Tools            | Benefit |
|-----------------------|-----------------------------|---------|
| Database              | BigQuery, Snowflake         | Handles large-scale data and concurrent users |
| Data Orchestration    | Airflow, dbt Cloud, Dagster | Automate dbt runs, alerts, and dependencies |
| Dashboard             | Looker, Superset, Metabase  | Production-grade dashboards with RBAC and performance |
| Streaming Ingestion   | Pub/Sub, Kafka, Fivetran    | Enable near real-time metrics pipelines |
| CI/CD                 | GitHub Actions, dbt tests   | Auto-validate models and deploy updates |

> âœ… If you're using BigQuery as your data warehouse, we recommend migrating your dashboards to **Looker** for long-term scalability, better data governance, and secure stakeholder access.

---

## âš™ï¸ Automation & Streaming Support

### CI/CD with GitHub Actions
- Set up workflows to:
  - Run `dbt build` and tests on push
  - Auto-deploy to Streamlit Cloud (or other cloud runners)
  - Send Slack/email alerts on failure

### Kafka for Real-time Ingestion (Simulated)
- Mock streaming data via a Kafka producer
- Write a listener script that:
  - Listens for new messages
  - Writes new rows to a CSV/parquet file in `data/`
  - Triggers a `dbt run` + auto-refresh Streamlit dashboard

> Add a `streaming/` folder for simulated producers and watchers

---

## ğŸ“¬ Questions?
Feel free to reach out or submit an issue. Happy building!

â€” Built with â¤ï¸ by the first Analytics Engineer at HeyMax





repo link. 
cd heymax-analytics


models/
â”œâ”€â”€ staging/
â”‚   â”œâ”€â”€ stg_events.sql
â”‚   â””â”€â”€ schema.yml
â”œâ”€â”€ intermediate/
â”‚   â”œâ”€â”€ dim_users.sql
â”‚   â”œâ”€â”€ fct_events.sql
â”‚   â”œâ”€â”€ user_lifecycle.sql
â”‚   â””â”€â”€ schema.yml
â”œâ”€â”€ marts/
â”‚   â”œâ”€â”€ growth_metrics.sql
â”‚   â””â”€â”€ schema.yml